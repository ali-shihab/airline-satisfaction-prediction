---
title: An R Markdown document converted from "~/projects/airline-satisfaction-prediction/full_pipeline.ipynb"
output: html_document
---

# Ali - Group 25 - Exploratory Data Analysis script
 This script contains the exploratory data analysis
 as a preliminary to the subsequent Modelling.R
 script. This was also assisted by the following
 functions from the PBA lab3DataPreprocessing.R script:
 - NPREPROCESSING_prettydataset()
 - NPREPROCESSING_outlier()
 - NPREPROCESSING_discreteNumeric()

# Setup

## Imports

```{r}
# clears console
cat("\014")

# seed for reproducibility
set.seed(123)

# clear all environment variables
rm(list=ls())
```

```{r}
# Loads the libraries
MYLIBRARIES<-c("outliers",
               "caret",
               "randomForest",
               "corrplot",
               "MASS",
               "formattable",
               "stats",
               "tidyr",
               "ggplot2",
               "GGally",
               "dplyr",
               "VIM",
               "PerformanceAnalytics",
               "Rtsne",
               "umap",
               "plotly",
               "dbscan",
               "stats",
               "rpart",
               "rpart.plot",
               "torch",
               "reshape2",
               "ggcorrplot",
               "Metrics",
               "pROC",
               "e1071",
               "rmarkdown",
               "psych"
               )
library(pacman)
pacman::p_load(char=MYLIBRARIES,install=TRUE,character.only=TRUE)

# load pre-processing helper functions
source("Preprocess.R")
source("lab3DataPrep.R")
```

## Definitions - Constants

```{r}
# ************************************************
# CONSTANTS
# ************************************************

# datasets - these were split into train and test
# by the original provider, and will be combined
TRAIN_FILENAME <- "train.csv"
TEST_FILENAME <- "test.csv"
TARGET_FIELD <- "satisfaction"
```

## Load Datasets

```{r}
  # read in data and combine
  data <- readData(TRAIN_FILENAME, TEST_FILENAME)
```

# Data Exploration & Preprocessing

## Descriptive Statistics

```{r}
  # descriptive structure & statistics
  str(data)
```

```{r}
 head(data)
```

```{r}
  # determine number of unique values of each field
  getUniqueValues(data)

  # determine field types
  fieldTypes<-getColumnTypes(data)
```

## Remove Duplicates

```{r}
filtered_data <- data[!duplicated(data), ]
```

```{r}
  str(data)
```

```{r}
  # determine number of unique values of each field
  getUniqueValues(filtered_data)
```

## Remove ID Fields

```{r}
data <- subset(data, select = -c(X, id))
```

```{r}
head(data)
```

```{r}
  # determine field types
  fieldTypes<-getColumnTypes(data)
```

```{r}
unique(data$Class)
```

## Missing Values

### Detection

```{r}
  # check missing values
  missing_values_summary <- colSums(is.na(data))
  print(missing_values_summary)
```

```{r}
  # check proportion of missing vals in each column
  missing_percentage <- colSums(is.na(
    data)) / nrow(data) * 100
  print(missing_percentage)
```

### Imputation

```{r}
data_with_known_delay <- data %>% filter(!is.na(ArrivalDelayinMinutes))
data_with_missing_delay <- data %>% filter(is.na(ArrivalDelayinMinutes))
```

```{r}
trainIndex <- createDataPartition(data_with_known_delay$ArrivalDelayinMinutes, p = 0.8, list = FALSE, times = 1)

# Create the train and test datasets
trainData <- data_with_known_delay[trainIndex, ]
testData <- data_with_known_delay[-trainIndex, ]
```

```{r}
# Function to impute median
#impute_median <- function(dataframe) {
#  dataframe %>%
#    mutate_if(is.numeric, function(x) {
#      ifelse(is.na(x), median(x, na.rm = TRUE), x)
#    })
#}

# Define the preprocessing method
preProcValues_data <- preProcess(trainData, method = "range")

# Transform the data using the scaling method
scaled_train <- predict(preProcValues_data, trainData)
scaled_test <- predict(preProcValues_data, testData)
```

```{r}
# Run linear regression
model <- lm(ArrivalDelayinMinutes ~ FlightDistance + DepartureDelayinMinutes + TypeofTravel + CustomerType, data = scaled_train)

# Train Rsquared
print(paste("train R-squared", summary(model)$r.squared))

# Predict missing values
predictions <- predict(model, newdata = scaled_test)

# R-squared
rss <- sum((scaled_test$ArrivalDelayinMinutes - predictions)^2)
tss <- sum((scaled_test$ArrivalDelayinMinutes - mean(scaled_test$ArrivalDelayinMinutes))^2)
r_squared <- 1 - rss/tss

print(paste("test R-squared", r_squared))

# Predict missing values
predicted_delays <- predict(model, newdata = predict(preProcValues_data, data_with_missing_delay))

# Define the preprocessing method
preProcValues_full_data <- preProcess(data %>% filter(!is.na(ArrivalDelayinMinutes)), method = "range")

# Transform the data using the scaling method
scaled <- predict(preProcValues_full_data, data)

# Impute the predicted values into the original dataset
scaled$ArrivalDelayinMinutes[is.na(scaled$ArrivalDelayinMinutes)] <- predicted_delays

print(predicted_delays)
```

```{r}
  # check proportion of missing vals in each column
  missing_percentage <- colSums(is.na(
    scaled)) / nrow(scaled) * 100
  print(missing_percentage)
```

## Convert categoricals to binary factors

```{r}
data <- scaled %>%
  mutate_if(is.character, as.factor)
```

```{r}
data <- data %>%
  mutate(across(where(~is.factor(.) && nlevels(.) == 2), ~as.numeric(as.factor(.)) - 1))
```

```{r}
str(data)
```

```{r}
# convert class to numeric
data <- data %>%
  mutate(Class = as.numeric(Class))
```

```{r}
str(data)
```

## Subsample 20,000 samples

```{r}
sampled <- sample_n(data, 20000)
```

## Min-Max Scale

```{r}
str(sampled)
```

```{r}
# Define the preprocessing method
preProcValues <- preProcess(sampled, method = "range")

# Transform the data using the scaling method
scaled <- predict(preProcValues, sampled)
```

```{r}
str(scaled)
```

# Visualisation

## Pairs - Continuous

```{r}
continuous <- scaled %>% select(Age, ArrivalDelayinMinutes, DepartureDelayinMinutes, FlightDistance)
ggpairs(continuous)
```

## Distributions

### Hist of Distributions

```{r}
  histPlots <-visualiseHist(data)
  print(histPlots)
```

## tSNA & UMAP 3D interactive visualisations

```{r}
# Run t-SNE
tsne_result <- Rtsne(scaled, dims = 3, perplexity = 43, max_iter = 500)

# Create a 3D interactive plot using plotly
plot_ly(x = tsne_result$Y[,1], y = tsne_result$Y[,2], z = tsne_result$Y[,3],
        type = 'scatter3d', 
        mode = 'markers', 
        marker = list(color = scaled$satisfaction, colorscale = 'Viridis', size = 5)) %>%
        layout(title = '3D tSNE Plot')
```

```{r}
# Run UMAP
umap_result <- umap(scaled, n_components = 3, n_neighbors = 43)

# Create a 3D interactive plot using plotly
plot_ly(x = umap_result$layout[,1], y = umap_result$layout[,2], z = umap_result$layout[,3],
        type = 'scatter3d', 
        mode = 'markers', 
        marker = list(color = scaled$satisfaction, colorscale = 'Viridis', size = 5)) %>%
        layout(title = '3D UMAP Plot')
```

# Outlier Detection - LOF

## Standardise (z-score)

```{r}
standardised <- scaled %>%
  mutate(across(-satisfaction,scale))
```

## Running a loop on this with varying neighbourhood sizes of 10 through 50

```{r}
# Outlier detection

# Initialize a dataframe to store outlier indices and corresponding k values
outliers_df <- data.frame(k = integer(), outlier_indices = I(list()))

# Loop over k values from 10 to 50
outlier_counts <- integer()

predictors <- select(standardised, -satisfaction)

for (k in 10:50) {
  lof_scores <- lof(predictors, k = k)
  
  # Identifying outliers (e.g., LOF score > 1.5)
  outlier_indices <- which(lof_scores > 1.5)
  outlier_counts[k - 9] <- length(outlier_indices)
  
  # Append to the dataframe
  outliers_df <- rbind(outliers_df, data.frame(k = k, outlier_indices = I(list(outlier_indices))))
  
  # Print the number of outliers for the current k
  cat("k =", k, ": Number of outliers =", length(outlier_indices), "\n")
}

# Plotting the curve of the number of outliers against each k value
ggplot(data = data.frame(k = 10:50, outliers = outlier_counts), aes(x = k, y = outliers)) +
  geom_line() +
  theme_minimal() +
  labs(title = "Number of Outliers vs. k-value in LOF",
       x = "k-value (Number of Neighbors)",
       y = "Number of Outliers")
```

## Filter for true outliers

```{r}
# Create a vector to hold the frequency of each index appearing as an outlier
index_frequencies <- list()

# Loop through each column in outliers_df to tally the outlier indices
for (k_value in 1:nrow(outliers_df)) {
  # each cell in the dataframe is a list of indices
  indices_list <- unlist(outliers_df[k_value, 2])
  for (idx in indices_list) {
    # Convert index to character to use as a name in the list
    index_char <- as.character(idx)
    if (index_char %in% names(index_frequencies)) {
      index_frequencies[[index_char]] <- index_frequencies[[index_char]] + 1
    } else {
      index_frequencies[[index_char]] <- 1
    }
  }
}

# Create a dataframe for plotting
frequency_df <- data.frame(Index = names(index_frequencies), Frequency = sapply(index_frequencies, identity)) %>%
  arrange(desc(Frequency))


# Plot the frequency of outlier indices
ggplot(frequency_df, aes(x = Index, y = Frequency)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Frequency of Data Points Being Labeled as Outliers",
       x = "Data Point Index",
       y = "Frequency")
```

```{r}
nrow(frequency_df)
```

```{r}
confident_outliers <- frequency_df %>% filter(Frequency >= 41*0.95)
print(nrow(confident_outliers))
```

```{r}
# Plot the frequency of outlier indices
ggplot(confident_outliers, aes(x = Index, y = Frequency)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Frequency of Data Points Being Labeled as Outliers",
       x = "Data Point Index",
       y = "Frequency")
```

```{r}

ggplot(confident_outliers, aes(x = Frequency)) +
  geom_histogram(binwidth = 1, fill = "black", color = "black") +
  theme_minimal() +
  labs(title = "Histogram of Outlier Frequencies",
       x = "Frequency of Being Labeled as Outlier",
       y = "Count of Data Point Indices")
```

## Sanity check on outliers

### 3D Visualisation

```{r}
# Create a binary outlier flag
outlier_flag_df <- predictors
outlier_flag_df$outlier <- seq_len(nrow(outlier_flag_df)) %in% as.integer(confident_outliers$Index)
```

```{r}
sum(outlier_flag_df$outlier)
```

### tSNE

```{r}
# Run t-SNE
tsne_outlier_result <- Rtsne(outlier_flag_df %>% select(-outlier), dims = 3, perplexity = 43, max_iter = 500)

# Create a 3D interactive plot using plotly
plot_ly(x = tsne_outlier_result$Y[,1], y = tsne_outlier_result$Y[,2], z = tsne_outlier_result$Y[,3],
        type = 'scatter3d', 
        mode = 'markers', 
        marker = list(color = outlier_flag_df$outlier, colorscale = 'Viridis', size = 5)) %>%
        layout(title = '3D tSNE Outlier Plot')
```

### UMAP

```{r}
# Run UMAP
umap_outlier_result <- umap(outlier_flag_df %>% select(-outlier), n_components = 3, n_neighbors = 43)

# Create a 3D interactive plot using plotly
plot_ly(x = umap_outlier_result$layout[,1], y = umap_outlier_result$layout[,2], z = umap_outlier_result$layout[,3],
        type = 'scatter3d', 
        mode = 'markers', 
        marker = list(color = outlier_flag_df$outlier, colorscale = 'Viridis', size = 5)) %>%
        layout(title = '3D tSNE Outlier Plot')
```

### Scatter - Flight Distance

```{r}
ggplot(outlier_flag_df, aes(x = outlier_flag_df$outlier, y = FlightDistance, color = outlier_flag_df$outlier)) +
  geom_point(alpha = 0.7) +
  theme_minimal() +
  labs(title = "Scatter Plot with Outliers Highlighted")
```

### Boxplot - Arrival Delay

```{r}
# For a single feature
ggplot(outlier_flag_df, aes(x = outlier_flag_df$outlier, y = ArrivalDelayinMinutes)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Box Plot for ArrivalDelayinMinutes by Outlier Flag")
```

### Hist & Dist plots

```{r}
# Histogram
ggplot(outlier_flag_df, aes(x = FlightDistance, fill = outlier_flag_df$outlier)) +
  geom_histogram(alpha = 0.6, bins = 30) +
  theme_minimal() +
  labs(title = "Histogram of Feature1 with Outliers")

# Density Plot
ggplot(outlier_flag_df, aes(x = DepartureDelayinMinutes, fill = outlier_flag_df$outlier)) +
  geom_density(alpha = 0.7) +
  theme_minimal() +
  labs(title = "Density Plot of Feature1 with Outliers")
```

### Remove outliers

```{r}
anomalies <- outlier_flag_df %>% filter(outlier)
cleaned <- standardised[!(standardised %in% anomalies)]
```

# Influence of Features

## Feature Signifcance with Decision Tree

```{r}
# Build the model
dt <- rpart(satisfaction ~ ., data = cleaned, method = "class")

# Print the model summary
print(dt)
```

```{r}
# Plot the decision tree
rpart.plot(dt, type = 4, extra = 2)
```

```{r}
# Extract feature importance
feature_importance <- dt$variable.importance

# Print the feature importance
print(feature_importance)

# Optionally, you can sort and plot the importance
sorted_importance <- sort(feature_importance, decreasing = TRUE)
barplot(sorted_importance, main = "Feature Importance in Decision Tree", las = 2)
```

## Plotting Dist. of Satisfaction by Variable

```{r}
# Define function to plot the distribution of satisfaction by variable
plot_satisfaction_by_variable <- function(data, variable, title) {
  formatted_variable <- gsub("\\.", " ", tolower(variable))
  
  temp_df <- data[, c('satisfaction', variable)]
  results <- as.data.frame(table(temp_df))
  
  ggplot(data = results, aes(x = satisfaction, y = Freq, fill = !!as.name(variable))) +
    geom_bar(stat = "identity", position = position_dodge(), alpha = 0.70) +
    geom_text(aes(label = Freq), fontface = "bold", vjust = 1.5,
              position = position_dodge(.9), size = 2.5) +
    labs(x = "\n Satisfaction", y = "Frequency\n", title = paste("\n Customer satisfaction based on", formatted_variable, "\n")) +
    theme(plot.title = element_text(hjust = 0.5),
          axis.title.x = element_text(face = "bold", colour = "black", size = 10),
          axis.title.y = element_text(face = "bold", colour = "black", size = 10),
          legend.title = element_text(face = "bold", size = 8))
}
```

```{r}
# plot by variable

# Visualise
variables <- names(cleaned)

# Set up the plotting area
par(mfrow = c(2, 4), pty = "m")
options(repr.plot.width = 10, repr.plot.height = 5)

# Loop through variables and generate plots
for (variable in variables) {
  plot <- plot_satisfaction_by_variable(cleaned, variable, variable)
  print(plot)
}
```

## Visualise Ratings Fields vs Cumulative Ratings Per Passenger

```{r}
# Count the number of unique values for each column
num_unique_values <- sapply(cleaned, function(col) length(unique(col)))

# Select columns that have 5 or 6 unique values (ratings columns)
sat_cols <- names(cleaned)[num_unique_values %in% c(5, 6)]
```

```{r}
# get all ratings data
sat_data <- cleaned[, sat_cols]

# find max occurrence of each feature
max_occurrence <- apply(sat_data, 1, function(x) names(which.max(table(x))))
max_occurrence_df <- data.frame(max_occurrence = max_occurrence)
max_sat <- cbind(max_occurrence_df, cleaned[, 'satisfaction', drop = FALSE])
results <- data.frame(table(max_sat))

# visualised the max occurrence of each feature against satisfaction column
ggplot(data = results, aes(x = satisfaction, y = Freq, fill = max_occurrence)) +
  geom_bar(stat = "identity", position = position_dodge(), alpha = 0.70) +
  
  geom_text(aes(label = Freq), fontface = "bold", vjust = 1.5,
            position = position_dodge(.9), size = 2.5) +
  labs(x = "\n satisfaction", y = "Frequency\n", title = "\n Customer satisfaction based on Maximum Occurrence of scale \n") +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face = "bold", colour = "black", size = 10),
        axis.title.y = element_text(face = "bold", colour = "black", size = 10),
        legend.title = element_text(face = "bold", size = 8))
```

# Dimensionality Reduction

## Factor Analysis & Derivation for Delay variables

```{r}
# Extract the delay features
delay_features <- cleaned %>% select(,c(DepartureDelayinMinutes, ArrivalDelayinMinutes))
```

```{r}
# Run factor analysis on the delay features, replacing with a single factor representing delay
fa_result <- fa(delay_features, nfactors = 1, fm = "minres")
delays <- fa_result$scores[, 1]

# Calculate the index where 'DepartureDelayinMinutes' is located
delay1_index <- which(names(cleaned) == "DepartureDelayinMinutes")

new_df <- data.frame(cleaned[1:(delay1_index - 1)],
                                DelayinMinutes = delays,
                                cleaned[(delay1_index+2):length(cleaned)])

# Insert the factor scores into the dataframe at the position of the delays
head(new_df)
```

```{r}
cleaned <- new_df
summary(cleaned)
```

## Autoencoder

### Data prep

```{r}
train_indices <- createDataPartition(cleaned$satisfaction, p = 0.8, list = FALSE, times = 1)
predictors <- cleaned %>% select(-satisfaction)
train_data <- predictors[train_indices, ]
test_data <- predictors[-train_indices, ]


data_tensor <- torch_tensor(as.matrix(predictors), dtype = torch_float32())

# Define the preprocessing method
preProc_cleaned <- preProcess(train_data, method = c("center", "scale"))

# Transform the data using the scaling method
train_data <- predict(preProc_cleaned, train_data)
test_data <- predict(preProc_cleaned, test_data)

train_tensor <- torch_tensor(as.matrix(train_data), dtype = torch_float32())
test_tensor <- torch_tensor(as.matrix(test_data), dtype = torch_float32())
```

### Model Architecture

```{r}
# Define the network architecture
autoencoder <- nn_module(
  initialize = function(layer_1, layer_2, layer_3, bottleneck) {
    self$encoder <- nn_sequential(
      nn_linear(21, layer_1),
      nn_batch_norm1d(layer_1),
      nn_relu(),
      nn_linear(layer_1, layer_2),
      nn_batch_norm1d(layer_2),
      nn_relu(),
      nn_linear(layer_2, layer_3),
      nn_batch_norm1d(layer_3),
      nn_relu(),
      nn_linear(layer_3, bottleneck),
      nn_batch_norm1d(bottleneck),
      nn_relu()
    )
    self$decoder <- nn_sequential(
      nn_linear(bottleneck, layer_3),
      nn_batch_norm1d(layer_3),
      nn_relu(),
      nn_linear(layer_3, layer_2),
      nn_batch_norm1d(layer_2),
      nn_relu(),
      nn_linear(layer_2, layer_1),
      nn_batch_norm1d(layer_1),
      nn_relu(),
      nn_linear(layer_1, 21),
      nn_relu(),
      nn_sigmoid()
    )
  },
  forward = function(x) {
    encoded <- self$encoder(x)
    decoded <- self$decoder(encoded)
    decoded
  }
)
```

### Training

```{r}
# models - varying latent space size
model_21 <- autoencoder(30, 40, 30, 21)
model_10 <- autoencoder(30, 25, 15, 10)
model_5 <- autoencoder(30, 20, 10, 5)
```

```{r}
# Define training logic
train <- function(model, train_tensor) {
  # Loss function
  loss_fn <- nn_mse_loss(reduction = "mean")

  # Optimizer
  optimizer <- optim_adam(model$parameters, lr = 0.001)

  # Training loop
  num_epochs <- 200
  batch_size <- 256
  # Calculate the number of batches
  num_batches <- ceiling(nrow(train_tensor) / batch_size)

  loss_list <- c(0)

  for (epoch in 1:num_epochs) {

    # Create batches
    train_data_batches <- torch_chunk(train_tensor, chunks = num_batches)

    for (batch in train_data_batches) {
      # Forward pass
      output <- model(batch)
      loss <- loss_fn(output, batch)
      
      # Backward and optimize
      optimizer$zero_grad()
      loss$backward()
      optimizer$step()
      
      append(loss_list, loss$item())
    }
    if (epoch %% 10 == 0) {
      cat("Epoch:", epoch, "Loss:", loss$item(), "\n")
    }
  }
  return(loss_list)
}
```

```{r}
loss_21 <- train(model_21, train_tensor)
loss_10 <- train(model_10, train_tensor)
loss_5 <- train(model_5, train_tensor)
```

```{r}
# Disable gradient calculations for testing
model_21$eval()
model_10$eval()
model_5$eval()

# Forward pass on the test set
test_output_21 <- model_21(test_tensor)
test_output_10 <- model_10(test_tensor)
test_output_5 <- model_5(test_tensor)

loss_fn <- nn_mse_loss(reduction = "mean")

# loss values
test_loss_21 <- loss_fn(test_output_21, test_tensor)$item()
test_loss_10 <- loss_fn(test_output_10, test_tensor)$item()
test_loss_5 <- loss_fn(test_output_5, test_tensor)$item()

cat("Model_21 test Loss (Reconstruction Error):", test_loss_21, "\n")
cat("Model_10 test Loss (Reconstruction Error):", test_loss_10, "\n")
cat("Model_5 test Loss (Reconstruction Error):", test_loss_5, "\n")
```

```{r}
# Gather feature names
feature_names <- names(predictors)

# convert to matrix
test_output_21 <- as.matrix(test_output_21$detach())
test_output_10 <- as.matrix(test_output_10$detach())
test_output_5 <- as.matrix(test_output_5$detach())

# Assign feature names to data tensor
colnames(test_output_21) <- feature_names
colnames(test_output_10) <- feature_names
colnames(test_output_5) <- feature_names

# Convert tensors to matrices
test_data_matrix <- as.matrix(test_tensor)
colnames(test_data_matrix) <- feature_names
standardised_output_21 <- predict(preProc_cleaned, test_output_21)
standardised_output_10 <- predict(preProc_cleaned, test_output_10)
standardised_output_5 <- predict(preProc_cleaned, test_output_5)
reconstructed_test_data_matrix_21 <- as.matrix(standardised_output_21)
reconstructed_test_data_matrix_10 <- as.matrix(standardised_output_10)
reconstructed_test_data_matrix_5 <- as.matrix(standardised_output_5)
correlation_21 <- cor(test_data_matrix, reconstructed_test_data_matrix_21)
correlation_10 <- cor(test_data_matrix, reconstructed_test_data_matrix_10)
correlation_5 <- cor(test_data_matrix, reconstructed_test_data_matrix_5)
```

```{r}
# Melt the correlation matrix
correlation_21_melted <- melt(correlation_21)
correlation_10_melted <- melt(correlation_10)
correlation_5_melted <- melt(correlation_5)

ggplot(correlation_21_melted, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  theme_minimal() +
  labs(title = "Correlation Heatmap", x = "Variable 1", y = "Variable 2") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
ggplot(correlation_10_melted, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  theme_minimal() +
  labs(title = "Correlation Heatmap", x = "Variable 1", y = "Variable 2") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(correlation_5_melted, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  theme_minimal() +
  labs(title = "Correlation Heatmap", x = "Variable 1", y = "Variable 2") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Visualisation of Latent Space - Pearson correlation & PCA

```{r}
# Extract encoded representations
encoded_data_21 <- model_21$encoder(data_tensor)
encoded_data_21 <- as.array(encoded_data_21$detach())
encoded_data_10 <- model_10$encoder(data_tensor)
encoded_data_10 <- as.array(encoded_data_10$detach())
encoded_data_5 <- model_5$encoder(data_tensor)
encoded_data_5 <- as.array(encoded_data_5$detach())

labels <- cleaned$satisfaction

# Visualization
plot_df_21 <- data.frame(encoded_data_21)
plot_df_10 <- data.frame(encoded_data_10)
plot_df_5 <- data.frame(encoded_data_5)
plot_df_21$satisfaction <- as.factor(labels)
plot_df_10$satisfaction <- as.factor(labels)
plot_df_5$satisfaction <- as.factor(labels)

ggplot(plot_df_21, aes(x = X20, y = X21, color = satisfaction)) +
  geom_point() +
  theme_minimal() +
  labs(title = "2D Visualization of the Latent Space", x = "Dimension 1", y = "Dimension 2")

ggplot(plot_df_10, aes(x = X9, y = X10, color = satisfaction)) +
  geom_point() +
  theme_minimal() +
  labs(title = "2D Visualization of the Latent Space", x = "Dimension 1", y = "Dimension 2")

ggplot(plot_df_5, aes(x = X4, y = X5, color = satisfaction)) +
  geom_point() +
  theme_minimal() +
  labs(title = "2D Visualization of the Latent Space", x = "Dimension 1", y = "Dimension 2")
```

```{r}
# Plot Pearson Correlation
cor_matrix_original <- cor(predictors)
ggcorrplot(cor_matrix_original, hc.order = TRUE)

# Plot Pearson Correlation
cor_matrix_21 <- cor(encoded_data_21)
ggcorrplot(cor_matrix_21, hc.order = TRUE)

# Plot Pearson Correlation
cor_matrix_10 <- cor(encoded_data_10)
ggcorrplot(cor_matrix_10, hc.order = TRUE)

# Plot Pearson Correlation
cor_matrix_5 <- cor(encoded_data_5)
ggcorrplot(cor_matrix_5, hc.order = TRUE)

# Run PCA on the encoded data
pca_result_original <- prcomp(predictors, center = TRUE, scale. = TRUE)
pca_data_original <- as.data.frame(pca_result_original$x[, 1:2])  # Taking first two principal components

# Run PCA on the encoded data
pca_result_21 <- prcomp(as.data.frame(encoded_data_21), center = TRUE, scale. = TRUE)
pca_data_21 <- as.data.frame(pca_result_21$x[, 1:2])  # Taking first two principal components

# Run PCA on the encoded data
pca_result_10 <- prcomp(as.data.frame(encoded_data_10), center = TRUE, scale. = TRUE)
pca_data_10 <- as.data.frame(pca_result_10$x[, 1:2])  # Taking first two principal components

# Run PCA on the encoded data
pca_result_5 <- prcomp(as.data.frame(encoded_data_5), center = TRUE, scale. = TRUE)
pca_data_5 <- as.data.frame(pca_result_5$x[, 1:2])  # Taking first two principal components

# Visualization
ggplot(pca_data_original, aes(x = PC1, y = PC2)) +
  geom_point() +
  theme_minimal() +
  labs(title = "PCA of the Encoded (Latent) Space", x = "PC1", y = "PC2")

# Visualization
ggplot(pca_data_21, aes(x = PC1, y = PC2)) +
  geom_point() +
  theme_minimal() +
  labs(title = "PCA of the Encoded (Latent) Space", x = "PC1", y = "PC2")

# Visualization
ggplot(pca_data_10, aes(x = PC1, y = PC2)) +
  geom_point() +
  theme_minimal() +
  labs(title = "PCA of the Encoded (Latent) Space", x = "PC1", y = "PC2")

# Visualization
ggplot(pca_data_5, aes(x = PC1, y = PC2)) +
  geom_point() +
  theme_minimal() +
  labs(title = "PCA of the Encoded (Latent) Space", x = "PC1", y = "PC2")
```

```{r}
# Visualization
ggplot(predictors, aes(x = Gender, y = CustomerType)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Gender vs CustomerType - Original", x = "Gender", y = "CustomerType")

# Visualization
ggplot(as.data.frame(standardised_output_21), aes(x = Gender, y = CustomerType)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Gender vs CustomerType - 22-embedded", x = "Gender", y = "CustomerType")
```

### Proportion of Variance Explained

```{r}
# Proportion of variance explained by each principal component
#variance_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)

# Cumulative variance explained
#cumulative_variance <- cumsum(variance_explained)
```

```{r}
# Create a dataframe for plotting
#variance_df <- data.frame(PC = seq_along(cumulative_variance), 
#                          CumulativeVariance = cumulative_variance)

# Plot
#ggplot(variance_df, aes(x = PC, y = CumulativeVariance)) +
#  geom_line() +
#  geom_point() +
#  scale_x_continuous(breaks = 1:length(cumulative_variance)) +
#  theme_minimal() +
#  labs(title = "Cumulative Variance Explained by PCA Components",
#       x = "Principal Component",
#       y = "Cumulative Variance Explained")
```

## UMAP Reduction to 10 dimensions

### Reduction

```{r}
# Run UMAP
umap_result <- umap(predictors, n_components = 10, n_neighbors = 43)

# Create a 3D interactive plot using plotly
plot_ly(x = umap_result$layout[,1], y = umap_result$layout[,2], z = umap_result$layout[,3],
        type = 'scatter3d', 
        mode = 'markers', 
        marker = list(color = cleaned$satisfaction, colorscale = 'Viridis', size = 5)) %>%
        layout(title = '3D UMAP Plot')
```

### Correlation Heatmap

```{r}
# Plot Pearson Correlation
cor_matrix_umap <- cor(umap_result$layout)
ggcorrplot(cor_matrix_umap, hc.order = TRUE)
```

